{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7258d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import (\n",
    "    FastNondominatedPartitioning,)\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.acquisition.multi_objective.monte_carlo import (\n",
    "    qExpectedHypervolumeImprovement)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "from cmim import CMIMFeatureSelector\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_mll as fit_gpytorch_model\n",
    "from botorch.optim.fit import fit_gpytorch_mll_scipy, fit_gpytorch_mll_torch\n",
    "\n",
    "from botorch.acquisition.analytic import ExpectedImprovement, UpperConfidenceBound\n",
    "from botorch.models.transforms import Normalize, Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_fully_bayesian_model_nuts\n",
    "from botorch.models.fully_bayesian import SaasFullyBayesianSingleTaskGP\n",
    "from gpytorch.kernels import MaternKernel\n",
    "\n",
    "from botorch.acquisition.analytic import ExpectedImprovement, UpperConfidenceBound, ConstrainedExpectedImprovement\n",
    "from botorch.acquisition import qExpectedImprovement, qUpperConfidenceBound\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import (\n",
    "    FastNondominatedPartitioning,\n",
    ")\n",
    "\n",
    "import pdb\n",
    "\n",
    "from botorch.acquisition.multi_objective.monte_carlo import (\n",
    "    qExpectedHypervolumeImprovement)\n",
    "\n",
    "from minepy import MINE\n",
    "from mordred import Calculator, descriptors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "class MolDAIS:\n",
    "    \n",
    "    def __init__(self, problem=None, optimizer_parameters=None, configuration=None, results=None):\n",
    "        self.problem = problem or MolDAIS.Problem()\n",
    "        self.optimizer_parameters = optimizer_parameters or MolDAIS.OptimizerParameters()\n",
    "        self.results = results or MolDAIS.Results()  # Initialize the Results class\n",
    "        self.configuration = configuration or MolDAIS.Configuration(self.problem, self.optimizer_parameters, self.results)\n",
    "        self.problemname = self.configuration.set_problemname()  # Automatically set the problemname\n",
    "        self.iteration = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    class Results:\n",
    "        \n",
    "        def __init__(self,X_full=torch.tensor([]), X=torch.tensor([]), y=torch.tensor([]), \n",
    "                    best_values=None, best_molecules=None):\n",
    "            self.X_full = X_full\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "            self.best_values = best_values or []  # To store the best objective value at each iteration\n",
    "            self.best_molecules = best_molecules or []  # To store the best molecule (SMILES) at each iteration\n",
    "\n",
    "\n",
    "\n",
    "    class Problem:\n",
    "        \n",
    "        def __init__(self, smiles_search_space=None, descriptors_search_space=None, targets=None, experiment_name='None'):\n",
    "            self.smiles_search_space = smiles_search_space  # pandas series of SMILES\n",
    "            self.init_smiles_search_space = smiles_search_space  # pandas series of SMILES\n",
    "            self.descriptors_search_space = descriptors_search_space  # torch.tensor for descriptors\n",
    "            self.targets = targets  # torch.tensor for targets\n",
    "            self.init_targets = targets  # torch.tensor for targets\n",
    "            self.experiment_name = experiment_name\n",
    "\n",
    "        def compute_descriptors(self):\n",
    "            calc = Calculator(descriptors, ignore_3D=False)\n",
    "            # Convert SMILES to RDKit Mol objects\n",
    "            mols = [Chem.MolFromSmiles(smile) for smile in self.smiles_search_space]\n",
    "            # Compute Mordred descriptors\n",
    "            df = calc.pandas(mols,quiet=True,)\n",
    "            # Remove non-numeric columns and columns with missing values\n",
    "            df = df.select_dtypes(include=['float64', 'int64']).dropna(axis=1)\n",
    "            # Convert the descriptors DataFrame to a torch tensor\n",
    "            self.descriptors_search_space = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "            \n",
    "            \n",
    "    class OptimizerParameters:\n",
    "        \n",
    "        def __init__(self, total_budget=100, initialization_budget=10, batch=1, sparsity_method='MIC',\n",
    "                     num_sparsity_feats=10, frac_sparsity_feats=0.01, multi_objective=False, constrained=False,\n",
    "                     acq_fun='EI', use_second_var=False, seed=0, custom_acq_fun=None, custom_sparsity_method=None,\n",
    "                     custom_model_type=None, custom_fit_strategy=None):\n",
    "            self.total_budget = total_budget\n",
    "            self.initialization_budget = initialization_budget\n",
    "            self.batch = batch\n",
    "            self.sparsity_method = sparsity_method\n",
    "            self.num_sparsity_feats = num_sparsity_feats\n",
    "            self.frac_sparsity_feats = frac_sparsity_feats\n",
    "            self.multi_objective = multi_objective\n",
    "            self.constrained = constrained\n",
    "            self.acq_fun = acq_fun\n",
    "            self.use_second_var = use_second_var\n",
    "            self.seed = seed\n",
    "            self.custom_acq_fun = custom_acq_fun\n",
    "            self.custom_sparsity_method = custom_sparsity_method\n",
    "            self.custom_model_type = custom_model_type\n",
    "            self.custom_fit_strategy = custom_fit_strategy\n",
    "            if (self.multi_objective or self.constrained):\n",
    "                self.use_second_var = True\n",
    "                print('using second variable')\n",
    "\n",
    "        def set_seed(self, seed):\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "            \n",
    "            \n",
    "    class Configuration:\n",
    "        \n",
    "        def __init__(self, problem, optimizer_parameters, results, model=None, sampled_smiles=None, problemname=None, iteration=0):\n",
    "            self.results = results  # Access results attributes via self.results\n",
    "            self.model = model\n",
    "            self.sampled_smiles = sampled_smiles or []\n",
    "            self.iteration = iteration\n",
    "            self.problem = problem  # Store the passed problem\n",
    "            self.optimizer_parameters = optimizer_parameters  # Store optimizer parameters\n",
    "            self.previous_filename = None\n",
    "            self.selected_indicies = None\n",
    "\n",
    "            sparsity_method = self.optimizer_parameters.sparsity_method\n",
    "            if sparsity_method == 'SAAS':\n",
    "                self.model_type = 'SAAS'\n",
    "            else:\n",
    "                self.model_type = self.optimizer_parameters.custom_model_type or 'GP'  # Default model type to GP\n",
    "\n",
    "        def set_problemname(self):\n",
    "            \"\"\"Set the problem name for saving and loading.\"\"\"\n",
    "            # Build a descriptive filename that includes the problem name, sparsity method, model type, acquisition function, and iteration\n",
    "            problem_name = self.problem.experiment_name or 'UnnamedProblem'\n",
    "            sparsity_method = self.optimizer_parameters.sparsity_method\n",
    "            model_type = self.model_type\n",
    "            acq_fun = self.optimizer_parameters.acq_fun\n",
    "            iteration_str = f\"iter_{self.iteration}\"\n",
    "            seed = self.optimizer_parameters.seed\n",
    "            num_feats = self.optimizer_parameters.num_sparsity_feats \n",
    "            # Generate descriptive filename\n",
    "            filename = f\"./results/New_{problem_name}_sparsity-{sparsity_method}{num_feats}_model-{model_type}_acq-{acq_fun}_{iteration_str}_seed-{seed}.pkl\"\n",
    "            return filename\n",
    "\n",
    "        def apply_sparsity(self):\n",
    "            \"\"\"Apply sparsity to the training data.\"\"\"\n",
    "            sampled_descriptors = self.results.X_full  # Access X_full from results\n",
    "            self.results.X = torch.zeros((self.results.y.shape[-1], sampled_descriptors.shape[0], self.optimizer_parameters.num_sparsity_feats))  # Update X in results\n",
    "            self.selected_indicies = torch.zeros((self.results.y.shape[-1], self.optimizer_parameters.num_sparsity_feats))\n",
    "            \n",
    "            if self.optimizer_parameters.sparsity_method == 'Spearman':\n",
    "                for j in range(self.results.y.shape[-1]):\n",
    "                    correlations = []\n",
    "                    for i in range(sampled_descriptors.shape[1]):\n",
    "                        try:\n",
    "                            corr, _ = torch.corrcoef(torch.stack([sampled_descriptors[:, i], self.results.y[:, j].squeeze()]))[0, 1]\n",
    "                        except:\n",
    "                            corr = 0\n",
    "                        correlations.append(corr)\n",
    "\n",
    "                    correlations = torch.abs(torch.tensor(correlations))\n",
    "                    top_indices = torch.argsort(correlations, descending=True)[:self.optimizer_parameters.num_sparsity_feats]\n",
    "\n",
    "                    self.results.X[j, :, :] = sampled_descriptors[:, top_indices].unsqueeze(0)\n",
    "                    self.selected_indicies[j,:] =top_indices.to(torch.int)\n",
    "\n",
    "            elif self.optimizer_parameters.sparsity_method == 'MIC':\n",
    "                mine = MINE(alpha=0.6, c=20)\n",
    "                for j in range(self.results.y.shape[-1]):\n",
    "                    mic_scores = []\n",
    "                    for i in range(sampled_descriptors.shape[1]):\n",
    "                        feature = sampled_descriptors[:, i].numpy()\n",
    "                        target = self.results.y[:, j].squeeze().numpy()\n",
    "                        mine.compute_score(feature, target)\n",
    "                        mic_scores.append(mine.mic())\n",
    "\n",
    "                    mic_scores = torch.tensor(mic_scores)\n",
    "                    top_indices = torch.argsort(mic_scores, descending=True)[:self.optimizer_parameters.num_sparsity_feats]\n",
    "\n",
    "                    self.results.X[j, :, :] = sampled_descriptors[:, top_indices].unsqueeze(0)\n",
    "                    self.selected_indicies[j,:] =top_indices.to(torch.int)\n",
    "\n",
    "                    \n",
    "                    \n",
    "            elif self.optimizer_parameters.sparsity_method == 'MI':\n",
    "                self.selected_indicies = torch.zeros((self.results.y.shape[-1], self.optimizer_parameters.num_sparsity_feats))\n",
    "\n",
    "                for j in range(self.results.y.shape[-1]):\n",
    "\n",
    "                    mi_scores = mutual_info_regression(sampled_descriptors.numpy(), self.results.y[:, j].numpy())\n",
    "                    k = self.optimizer_parameters.num_sparsity_feats\n",
    "                    top_indices = np.argsort(arr)[-k:][::-1]\n",
    "\n",
    "                    self.selected_indicies[j, :] = torch.tensor(top_indices)\n",
    "                    self.results.X[j, :, :] = sampled_descriptors[:, top_indices].unsqueeze(0)\n",
    "\n",
    "\n",
    "            elif self.optimizer_parameters.sparsity_method == 'CMI':\n",
    "                for j in range(self.results.y.shape[-1]):\n",
    "                    selector = CMIMFeatureSelector(n_features_to_select=self.optimizer_parameters.num_sparsity_feats,task='regression')\n",
    "                    selector.fit(sampled_descriptors.numpy(), self.results.y[:, j].numpy())\n",
    "                    X_transformed = selector.transform(sampled_descriptors)                                    \n",
    "                    try: self.results.X[j,:,:] = torch.tensor(X_transformed)\n",
    "                    except: pdb.set_trace()\n",
    "\n",
    "            elif self.optimizer_parameters.sparsity_method == 'SAAS':\n",
    "                # Use the SAAS method for sparsity\n",
    "                self.results.X = torch.zeros((self.results.y.shape[-1], sampled_descriptors.shape[0], sampled_descriptors.shape[1]))  # Update X in results\n",
    "                self.selected_indicies = torch.zeros((self.results.y.shape[-1],sampled_descriptors.shape[-1] ))\n",
    "            \n",
    "                for j in range(self.results.y.shape[-1]):\n",
    "                  self.results.X[j,:,:] = sampled_descriptors\n",
    "                  self.selected_indicies[j,:] = torch.arange(sampled_descriptors.shape[-1]).to(torch.int)\n",
    "\n",
    "                    \n",
    "                    \n",
    "            return self.results.X\n",
    "\n",
    "        def create_model(self):\n",
    "            \"\"\"Create and return a model depending on the selected type: GP or SAAS-GP.\"\"\"\n",
    "            model_type = self.model_type  # Use the provided model type or default to 'GP'\n",
    "\n",
    "            if self.optimizer_parameters.use_second_var:\n",
    "                if model_type == 'GP':\n",
    "                    model = []\n",
    "                    mll = []\n",
    "                    for i in range(self.results.y.shape[-1]):\n",
    "                        model.append(SingleTaskGP(train_X=self.results.X[i,:, :], \n",
    "                                                  train_Y=self.results.y[:, i].unsqueeze(1),\n",
    "                                                  input_transform=Normalize(d=self.results.X.size(-1)), \n",
    "                                                  outcome_transform=Standardize(m=1), \n",
    "                                                  covar_module=MaternKernel(nu=2.5, ard_num_dims=self.results.X.shape[-1])  # Using Matern 2.5 kernel with ARD\n",
    "                                                  ))\n",
    "                        mll.append(ExactMarginalLogLikelihood(likelihood=model[i].likelihood, model=model[i]))\n",
    "                    self.model = model\n",
    "                    self.mll = mll\n",
    "                elif model_type == 'SAAS':\n",
    "                    model = []\n",
    "                    for i in range(self.results.y.shape[-1]):\n",
    "                        model.append(SaasFullyBayesianSingleTaskGP(train_X=self.results.X[i, :, :], train_Y=self.results.y[:, i].unsqueeze(1),\n",
    "                                                                  input_transform=Normalize(d=self.results.X.size(-1)), outcome_transform=Standardize(m=1)))\n",
    "                    self.model = model\n",
    "                    self.mll = None\n",
    "            else:\n",
    "                if model_type == 'GP':\n",
    "                    # Standard GP model\n",
    "                    self.model = SingleTaskGP(\n",
    "                        train_X=self.results.X[0, :, :],  # Access X from results\n",
    "                        train_Y=self.results.y,  # Access y from results\n",
    "                        input_transform=Normalize(d=self.results.X.shape[-1]),  # Normalize input features\n",
    "                        outcome_transform=Standardize(m=1),   # Standardize the outputs\n",
    "                        covar_module=MaternKernel(nu=2.5, ard_num_dims=self.results.X.shape[-1])  # Using Matern 2.5 kernel with ARD\n",
    "\n",
    "                    )\n",
    "                    self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
    "\n",
    "                elif model_type == 'SAAS':\n",
    "                    # SAAS-GP model\n",
    "                    self.model = SaasFullyBayesianSingleTaskGP(\n",
    "                        train_X=self.results.X[0, :, :],  # Access X from results\n",
    "                        train_Y=self.results.y,  # Access y from results\n",
    "                        input_transform=Normalize(d=self.results.X.shape[-1]),  # Normalize input features\n",
    "                        outcome_transform=Standardize(m=1)  # Standardize the outputs\n",
    "                    )\n",
    "                    self.mll = None  # SAAS-GP uses a fully Bayesian fitting approach, no marginal likelihood\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "        def fit_model(self):\n",
    "            \"\"\"Fit the model based on its type.\"\"\"\n",
    "            model_type = self.model_type  # Use the provided model type or default to 'GP'\n",
    "\n",
    "            if model_type == 'GP':\n",
    "                if not self.optimizer_parameters.use_second_var:\n",
    "                    try:\n",
    "                        with gpytorch.settings.cholesky_max_tries(100):\n",
    "                            fit_gpytorch_model(self.mll)\n",
    "                    except:\n",
    "                        try: \n",
    "                            fit_gpytorch_mll_scipy(self.mll) \n",
    "                        except:\n",
    "                            fit_gpytorch_mll_torch(self.mll)\n",
    "                    \n",
    "                else:\n",
    "                    for mll_i in self.mll:\n",
    "                        fit_gpytorch_model(mll_i)\n",
    "                    self.model = ModelListGP(*self.model)\n",
    "\n",
    "            elif model_type == 'SAAS':\n",
    "                warmup_steps = 12\n",
    "                num_samples = 25\n",
    "                thinning = 16              \n",
    "                if not self.optimizer_parameters.use_second_var:\n",
    "                    fit_fully_bayesian_model_nuts(\n",
    "                        self.model,\n",
    "                        warmup_steps=warmup_steps,  # Number of warmup steps\n",
    "                        num_samples=num_samples,   # Number of MCMC samples\n",
    "                        thinning=thinning  # Thinning rate\n",
    "                    )\n",
    "                    self.model = self.model\n",
    "                else:\n",
    "                    for model_i in self.model:\n",
    "                        fit_fully_bayesian_model_nuts(model_i, warmup_steps=warmup_steps, num_samples=num_samples, thinning=thinning, disable_progbar=False)\n",
    "                    self.model = ModelListGP(*self.model)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "        def get_sample(self):\n",
    "            \"\"\"Get the next sample based on the selected acquisition function.\"\"\"\n",
    "            X_space = self.problem.descriptors_search_space[:, self.selected_indicies.to(torch.int)].unsqueeze(1).to(torch.float)\n",
    "            if self.optimizer_parameters.use_second_var:\n",
    "                dim = self.results.X.shape[1]  # Access X from results\n",
    "                if self.optimizer_parameters.multi_objective:\n",
    "                    try:\n",
    "                        partitioning = partitioning = FastNondominatedPartitioning(ref_point=torch.max(self.results.y, dim=0).values,Y=self.results.y)\n",
    "                    except:\n",
    "                        pdb.set_trace()\n",
    "                    try: del f_acq\n",
    "                    except: pass\n",
    "                    f_acq = qExpectedHypervolumeImprovement(\n",
    "                        model=self.model,\n",
    "                        ref_point=torch.max(self.results.y[:,0]*self.results.y[:,1], dim=0),\n",
    "                        partitioning=partitioning, )\n",
    "\n",
    "                    acq = safe_eval(f_acq, X_space) \n",
    "                    max_acq = torch.max(acq)\n",
    "                    max_acq_index = torch.argmax(acq)\n",
    "                    next_sample_index = max_acq_index\n",
    "\n",
    "                else:  # constrained\n",
    "                    lwr_bound = 0\n",
    "                    feas_vals = self.results.y[:, 1] <= lwr_bound  # Access y from results\n",
    "                    try:\n",
    "                        max_val = (self.results.y[:, 0] * feas_vals).max()  # Access y from results\n",
    "                    except:\n",
    "                        pdb.set_trace()\n",
    "                    constraints = {1: (lwr_bound, None)}\n",
    "                    f_acq = ConstrainedExpectedImprovement(self.model, max_val, 0, constraints)\n",
    "\n",
    "                    acq = safe_eval(f_acq, X_space)  # Access X from results\n",
    "                    max_acq = torch.max(acq)\n",
    "                    max_acq_index = torch.argmax(acq)\n",
    "                    next_sample_index = max_acq_index\n",
    "\n",
    "            else: # single variable\n",
    "                if self.optimizer_parameters.acq_fun == 'EI':\n",
    "                    acq_function = ExpectedImprovement(self.model, best_f=torch.max(self.results.y))  # Access y from results\n",
    "                elif self.optimizer_parameters.acq_fun == 'UCB':\n",
    "                    acq_function = UpperConfidenceBound(self.model, beta=2.0)\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown acquisition function\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    try: acq_vals = acq_function(X_space)  # Access X from results\n",
    "                    except: pdb.set_trace()\n",
    "                    next_sample_index = torch.argmax(acq_vals).item()\n",
    "                    \n",
    "            return next_sample_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def save_iteration(self):\n",
    "            \"\"\"Save the current state of optimization.\"\"\"\n",
    "            iteration_data = {\n",
    "                'X_full': self.results.X_full,\n",
    "                'X': self.results.X,\n",
    "                'y': self.results.y,\n",
    "                'model': self.model,\n",
    "                'best_values': self.results.best_values,\n",
    "                'best_molecules': self.results.best_molecules,\n",
    "                'iteration': self.iteration,\n",
    "                'targets': self.problem.targets,\n",
    "                'sampled_smiles': self.sampled_smiles\n",
    "            }\n",
    "            filename = self.set_problemname()\n",
    "            if self.previous_filename is not None and os.path.exists(self.previous_filename):\n",
    "                os.remove(self.previous_filename)\n",
    "            \n",
    "            with open(filename, 'wb') as f:\n",
    "                torch.save(iteration_data, f)\n",
    "                \n",
    "            self.previous_filename = filename\n",
    "            \n",
    "            print(f\"Iteration {self.iteration} saved as {filename}\")\n",
    "\n",
    "        def load_from_checkpoint(self, checkpoint_filename):\n",
    "            \"\"\"Load the state of optimization from a saved checkpoint.\"\"\"\n",
    "            with open(checkpoint_filename, 'rb') as f:\n",
    "                checkpoint_data = torch.load(f)\n",
    "            self.results.X_full = checkpoint_data['X_full']\n",
    "            self.results.X = checkpoint_data['X']\n",
    "            self.results.y = checkpoint_data['y']\n",
    "            self.model = checkpoint_data['model']\n",
    "            self.results.best_values = checkpoint_data['best_values']\n",
    "            self.results.best_molecules = checkpoint_data['best_molecules']\n",
    "            self.iteration = checkpoint_data['iteration']\n",
    "            self.problem.targets = checkpoint_data['targets']\n",
    "            self.sampled_smiles = checkpoint_data['sampled_smiles']\n",
    "            print(f\"Loaded checkpoint from {checkpoint_filename}\")\n",
    "\n",
    "\n",
    "        def get_max_y(self, y, all=False):\n",
    "          if self.optimizer_parameters.multi_objective:\n",
    "            y_ = torch.prod(y, dim=1)\n",
    "            return max(y_) if not all else y_\n",
    "\n",
    "          elif self.optimizer_parameters.constrained:\n",
    "            feas = self.results.y[:,1]<=0\n",
    "            return max(self.results.y[:,0]*feas)  if not all else self.results.y[:,0]*feas\n",
    "          else:\n",
    "            return max(self.results.y) if not all else self.results.y\n",
    "\n",
    "\n",
    "\n",
    "        def optimize(self):\n",
    "            \"\"\"Run the optimization loop.\"\"\"\n",
    "            # Set the seed for reproducibility\n",
    "            seed = self.optimizer_parameters.seed\n",
    "\n",
    "            total_budget = self.optimizer_parameters.total_budget\n",
    "            init_budget = self.optimizer_parameters.initialization_budget\n",
    "            \n",
    "\n",
    "            # Check if training data is empty\n",
    "            if self.results.X.numel() == 0:  # Check if X (training data) is empty\n",
    "                print(f\"Initializing with random {init_budget} samples from the search space.\")\n",
    "\n",
    "                # Randomly sample from the search space using initialization budget\n",
    "                np.random.seed(seed)\n",
    "                init_indices = np.random.choice(np.arange(len(self.problem.smiles_search_space)), init_budget, replace=False)\n",
    "\n",
    "                # Add sampled SMILES and corresponding descriptors/targets to the training data\n",
    "                sampled_smiles = [self.problem.smiles_search_space[idx] for idx in init_indices]\n",
    "                self.results.X_full = self.problem.descriptors_search_space[init_indices]\n",
    "                self.results.y = self.problem.targets[init_indices]\n",
    "                self.results.X = self.apply_sparsity()\n",
    "\n",
    "                # Remove the sampled data from the search space and targets\n",
    "                self.problem.smiles_search_space = [smile for i, smile in enumerate(self.problem.smiles_search_space) if i not in init_indices]\n",
    "                self.problem.descriptors_search_space = torch.cat([self.problem.descriptors_search_space[i].unsqueeze(0) for i in range(len(self.problem.descriptors_search_space)) if i not in init_indices])\n",
    "                self.problem.targets = torch.cat([self.problem.targets[i].unsqueeze(0) for i in range(len(self.problem.targets)) if i not in init_indices])\n",
    "\n",
    "                # Store the sampled SMILES for reference\n",
    "                self.sampled_smiles.extend(sampled_smiles)\n",
    "\n",
    "            best_value = self.get_max_y(self.results.y).item()\n",
    "            best_smiles = self.sampled_smiles[torch.argmax(self.get_max_y(self.results.y, all=True))]\n",
    "\n",
    "            for bo_iter in range(self.iteration, total_budget - init_budget):\n",
    "                self.iteration = bo_iter\n",
    "                print(f\"Starting BO Iteration: {bo_iter + init_budget} / {total_budget}\")\n",
    "                self.create_model()\n",
    "                self.fit_model()\n",
    "\n",
    "                # Get the next sample\n",
    "                next_sample_idx = self.get_sample()\n",
    "\n",
    "                next_sample_value = self.problem.targets[next_sample_idx, :]\n",
    "                \n",
    "                \n",
    "                if self.optimizer_parameters.use_second_var:\n",
    "                    next_score = self.get_max_y(next_sample_value.unsqueeze(0))\n",
    "                else:\n",
    "                    next_score = next_sample_value\n",
    "\n",
    "                # Track the best objective value and the corresponding molecule\n",
    "                if next_score > best_value:\n",
    "                    best_value = next_score\n",
    "                    best_smiles = self.problem.smiles_search_space[next_sample_idx]\n",
    "                try:\n",
    "                    best_value = best_value.item()\n",
    "                except:\n",
    "                    pass\n",
    "                self.results.best_values.append(best_value)\n",
    "                self.results.best_molecules.append(best_smiles)\n",
    "\n",
    "                # Update training data (X, y) with the new sample\n",
    "                new_X_sample = self.problem.descriptors_search_space[next_sample_idx].unsqueeze(0)\n",
    "                new_y_sample = self.problem.targets[next_sample_idx].unsqueeze(0)\n",
    "                self.results.X_full = torch.cat((self.results.X_full, new_X_sample), dim=0)\n",
    "                self.results.y = torch.cat((self.results.y, new_y_sample), dim=0)\n",
    "                self.apply_sparsity()\n",
    "\n",
    "                # Remove the selected sample from the search space and targets\n",
    "                self.problem.smiles_search_space.pop(next_sample_idx)\n",
    "                self.problem.descriptors_search_space = torch.cat([self.problem.descriptors_search_space[i].unsqueeze(0) for i in range(len(self.problem.descriptors_search_space)) if i != next_sample_idx], dim=0)\n",
    "                self.problem.targets = torch.cat([self.problem.targets[i].unsqueeze(0) for i in range(len(self.problem.targets)) if i != next_sample_idx], dim=0)\n",
    "\n",
    "                # Store the sampled SMILES for reference\n",
    "                self.sampled_smiles.append(best_smiles)\n",
    "\n",
    "                self.iteration = bo_iter + 1\n",
    "                \n",
    "                self.save_iteration()\n",
    "\n",
    "\n",
    "        def plot_convergence(self):\n",
    "            \"\"\"Plot the best objective value at each iteration and display the best molecules.\"\"\"\n",
    "            zo = 0\n",
    "            rd_opts = Draw.DrawingOptions()\n",
    "            rd_opts.bgColor=None\n",
    "\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "            # Plot the convergence curve (best objective values)\n",
    "            ax.plot(range(len(self.results.best_values)), self.results.best_values, marker='o', label='Best Objective Value')\n",
    "            #ax.set_title('Convergence Plot of Best Objective Value')\n",
    "            ax.set_xlabel('Iteration')\n",
    "            ax.set_ylabel('Best Objective Value')\n",
    "            ax.grid(True)\n",
    "\n",
    "            top_row = self.get_max_y(self.problem.init_targets)\n",
    "            bottom_row = min(self.get_max_y(self.problem.init_targets, all =True))\n",
    "           \n",
    "            plt.plot([0,len(self.results.best_values)], [top_row, top_row],'k:', label='Dataset min/max') \n",
    "            plt.plot([0,len(self.results.best_values)], [bottom_row, bottom_row],'k:') \n",
    "\n",
    "            # Draw molecules on the plot at various points --incomplete\n",
    "            # - does not look good in many cases \n",
    "            # - consider changing frequency and size of molecule images\n",
    "            if 0:\n",
    "              value_ = -1E12\n",
    "              for i, (value, smiles) in enumerate(zip(self.results.best_values, self.results.best_molecules)):\n",
    "                  if value>value_:  \n",
    "                      mol = Chem.MolFromSmiles(smiles)\n",
    "                      if mol:\n",
    "                          img = Draw.MolToImage(mol,options=rd_opts)\n",
    "                          # Display the molecule image on the plot\n",
    "                          imgbox = OffsetImage(img, zoom=0.25)\n",
    "                          ab = AnnotationBbox(imgbox, (i, top_row), frameon=False, zorder=zo)\n",
    "                          ax.add_artist(ab)\n",
    "                  value_=value\n",
    "            \n",
    "            ax.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.ylim(bottom_row*.9,top_row*1.1)\n",
    "            plt.xlim(0,len(self.results.best_values)-1)\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# Helper function to update the test/train data\n",
    "def update_test_train_data(next_sample_idx, smiles_search_space, descriptors_search_space):\n",
    "    new_smiles_space = smiles_search_space[:next_sample_idx] + smiles_search_space[next_sample_idx + 1:]\n",
    "    new_descriptors = torch.cat((descriptors_search_space[:next_sample_idx], descriptors_search_space[next_sample_idx + 1:]), dim=0)\n",
    "    return new_smiles_space, new_descriptors\n",
    "\n",
    "\n",
    "def safe_eval(f, x):\n",
    "  n = len(x)\n",
    "  nm = 10\n",
    "  splits = int(np.ceil(n/nm))\n",
    "  obs = []\n",
    "  for i in range(splits):\n",
    "    if i+1 != splits:\n",
    "      xi = x[nm*i:nm*(i+1)].detach()\n",
    "    else:\n",
    "      xi = x[nm*i:].detach()\n",
    "    try:obs.append(f(xi).detach().tolist())\n",
    "    except: pdb.set_trace()\n",
    "  obs = torch.tensor(sum(obs, []))\n",
    "  return obs\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b94824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = 'dG'; data_file = 'MORDRED_SMILES_dft_Gsolv.csv'\n",
    "df_dg = pd.read_csv('../../prop_data/'+data_file)#.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "name = 'E'; data_file = 'MORDRED_SMILES_dft_E.csv'\n",
    "df_e0 = pd.read_csv('../../prop_data/'+data_file)#.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import numpy as np\n",
    "\n",
    "def compute_molecular_weights(smiles_list):\n",
    "    molecular_weights = []\n",
    "    for smiles in smiles_list:\n",
    "        molecule = Chem.MolFromSmiles(smiles)\n",
    "        if molecule:\n",
    "            mw = Descriptors.MolWt(molecule)\n",
    "            molecular_weights.append(mw)\n",
    "        else:\n",
    "            molecular_weights.append(np.nan)  # Use NaN for invalid SMILES\n",
    "    return np.array(molecular_weights)\n",
    "\n",
    "smiles_list = df_dg['SMILES'].to_list()\n",
    "molecular_weights = compute_molecular_weights(smiles_list)\n",
    "\n",
    "\n",
    "# Step 1: load data\n",
    "smiles_list = df_dg['SMILES'].to_list()\n",
    "values1 = -1*torch.tensor(df_dg['Gsolv'].values, dtype=torch.float32).reshape(-1,1)\n",
    "F = 96485\n",
    "values2 = torch.tensor(((df_e0['E'].values+0.76)*2*F)/ (3.600*molecular_weights), dtype=torch.float32).reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "values = torch.cat([values1,values2], dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9d6131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['SMILES', 'Gsolv', 'nAcid', 'nBase'], dtype='object'),\n",
       " Index(['SMILES', 'E', 'ABC', 'ABCGG'], dtype='object'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dg.columns[:4], df_e0.columns[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534349f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: load data\n",
    "smiles_list = df_dg['SMILES'].to_list()\n",
    "values1 = -1*torch.tensor(df_dg['Gsolv'].values, dtype=torch.float32).reshape(-1,1)\n",
    "F = 96485\n",
    "values2 = torch.tensor(((df_e0['E'].values+0.76)*2*F)/ (3.600*molecular_weights), dtype=torch.float32).reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "values = torch.cat([values1,values2], dim=1)\n",
    "\n",
    "\n",
    "descriptors_search_space = torch.tensor(df_dg.iloc[:,2:].select_dtypes(include=['float64', 'int64']).dropna(axis=1).values)\n",
    "                          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e86ed99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 10 samples from the search space.\n",
      "Starting BO Iteration: 10 / 100\n",
      "Iteration 1 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_1_seed-0.pkl\n",
      "Starting BO Iteration: 11 / 100\n",
      "Iteration 2 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_2_seed-0.pkl\n",
      "Starting BO Iteration: 12 / 100\n",
      "Iteration 3 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_3_seed-0.pkl\n",
      "Starting BO Iteration: 13 / 100\n",
      "Iteration 4 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_4_seed-0.pkl\n",
      "Starting BO Iteration: 14 / 100\n",
      "Iteration 5 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_5_seed-0.pkl\n",
      "Starting BO Iteration: 15 / 100\n",
      "Iteration 6 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_6_seed-0.pkl\n",
      "Starting BO Iteration: 16 / 100\n",
      "Iteration 7 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_7_seed-0.pkl\n",
      "Starting BO Iteration: 17 / 100\n",
      "Iteration 8 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_8_seed-0.pkl\n",
      "Starting BO Iteration: 18 / 100\n",
      "Iteration 9 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_9_seed-0.pkl\n",
      "Starting BO Iteration: 19 / 100\n",
      "Iteration 10 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_10_seed-0.pkl\n",
      "Starting BO Iteration: 20 / 100\n",
      "Iteration 11 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_11_seed-0.pkl\n",
      "Starting BO Iteration: 21 / 100\n",
      "Iteration 12 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_12_seed-0.pkl\n",
      "Starting BO Iteration: 22 / 100\n",
      "Iteration 13 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_13_seed-0.pkl\n",
      "Starting BO Iteration: 23 / 100\n",
      "Iteration 14 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_14_seed-0.pkl\n",
      "Starting BO Iteration: 24 / 100\n",
      "Iteration 15 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_15_seed-0.pkl\n",
      "Starting BO Iteration: 25 / 100\n",
      "Iteration 16 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_16_seed-0.pkl\n",
      "Starting BO Iteration: 26 / 100\n",
      "Iteration 17 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_17_seed-0.pkl\n",
      "Starting BO Iteration: 27 / 100\n",
      "Iteration 18 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_18_seed-0.pkl\n",
      "Starting BO Iteration: 28 / 100\n",
      "Iteration 19 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_19_seed-0.pkl\n",
      "Starting BO Iteration: 29 / 100\n",
      "Iteration 20 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_20_seed-0.pkl\n",
      "Starting BO Iteration: 30 / 100\n",
      "Iteration 21 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_21_seed-0.pkl\n",
      "Starting BO Iteration: 31 / 100\n",
      "Iteration 22 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_22_seed-0.pkl\n",
      "Starting BO Iteration: 32 / 100\n",
      "Iteration 23 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_23_seed-0.pkl\n",
      "Starting BO Iteration: 33 / 100\n",
      "Iteration 24 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_24_seed-0.pkl\n",
      "Starting BO Iteration: 34 / 100\n",
      "Iteration 25 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_25_seed-0.pkl\n",
      "Starting BO Iteration: 35 / 100\n",
      "Iteration 26 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_26_seed-0.pkl\n",
      "Starting BO Iteration: 36 / 100\n",
      "Iteration 27 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_27_seed-0.pkl\n",
      "Starting BO Iteration: 37 / 100\n",
      "Iteration 28 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_28_seed-0.pkl\n",
      "Starting BO Iteration: 38 / 100\n",
      "Iteration 29 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_29_seed-0.pkl\n",
      "Starting BO Iteration: 39 / 100\n",
      "Iteration 30 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_30_seed-0.pkl\n",
      "Starting BO Iteration: 40 / 100\n",
      "Iteration 31 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_31_seed-0.pkl\n",
      "Starting BO Iteration: 41 / 100\n",
      "Iteration 32 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_32_seed-0.pkl\n",
      "Starting BO Iteration: 42 / 100\n",
      "Iteration 33 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_33_seed-0.pkl\n",
      "Starting BO Iteration: 43 / 100\n",
      "Iteration 34 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_34_seed-0.pkl\n",
      "Starting BO Iteration: 44 / 100\n",
      "Iteration 35 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_35_seed-0.pkl\n",
      "Starting BO Iteration: 45 / 100\n",
      "Iteration 36 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_36_seed-0.pkl\n",
      "Starting BO Iteration: 46 / 100\n",
      "Iteration 37 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_37_seed-0.pkl\n",
      "Starting BO Iteration: 47 / 100\n",
      "Iteration 38 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_38_seed-0.pkl\n",
      "Starting BO Iteration: 48 / 100\n",
      "Iteration 39 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_39_seed-0.pkl\n",
      "Starting BO Iteration: 49 / 100\n",
      "Iteration 40 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_40_seed-0.pkl\n",
      "Starting BO Iteration: 50 / 100\n",
      "Iteration 41 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_41_seed-0.pkl\n",
      "Starting BO Iteration: 51 / 100\n",
      "Iteration 42 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_42_seed-0.pkl\n",
      "Starting BO Iteration: 52 / 100\n",
      "Iteration 43 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_43_seed-0.pkl\n",
      "Starting BO Iteration: 53 / 100\n",
      "Iteration 44 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_44_seed-0.pkl\n",
      "Starting BO Iteration: 54 / 100\n",
      "Iteration 45 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_45_seed-0.pkl\n",
      "Starting BO Iteration: 55 / 100\n",
      "Iteration 46 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_46_seed-0.pkl\n",
      "Starting BO Iteration: 56 / 100\n",
      "Iteration 47 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_47_seed-0.pkl\n",
      "Starting BO Iteration: 57 / 100\n",
      "Iteration 48 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_48_seed-0.pkl\n",
      "Starting BO Iteration: 58 / 100\n",
      "Iteration 49 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_49_seed-0.pkl\n",
      "Starting BO Iteration: 59 / 100\n",
      "Iteration 50 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_50_seed-0.pkl\n",
      "Starting BO Iteration: 60 / 100\n",
      "Iteration 51 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_51_seed-0.pkl\n",
      "Starting BO Iteration: 61 / 100\n",
      "Iteration 52 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_52_seed-0.pkl\n",
      "Starting BO Iteration: 62 / 100\n",
      "Iteration 53 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_53_seed-0.pkl\n",
      "Starting BO Iteration: 63 / 100\n",
      "Iteration 54 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_54_seed-0.pkl\n",
      "Starting BO Iteration: 64 / 100\n",
      "Iteration 55 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_55_seed-0.pkl\n",
      "Starting BO Iteration: 65 / 100\n",
      "Iteration 56 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_56_seed-0.pkl\n",
      "Starting BO Iteration: 66 / 100\n",
      "Iteration 57 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_57_seed-0.pkl\n",
      "Starting BO Iteration: 67 / 100\n",
      "Iteration 58 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_58_seed-0.pkl\n",
      "Starting BO Iteration: 68 / 100\n",
      "Iteration 59 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_59_seed-0.pkl\n",
      "Starting BO Iteration: 69 / 100\n",
      "Iteration 60 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_60_seed-0.pkl\n",
      "Starting BO Iteration: 70 / 100\n",
      "Iteration 61 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_61_seed-0.pkl\n",
      "Starting BO Iteration: 71 / 100\n",
      "Iteration 62 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_62_seed-0.pkl\n",
      "Starting BO Iteration: 72 / 100\n",
      "Iteration 63 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_63_seed-0.pkl\n",
      "Starting BO Iteration: 73 / 100\n",
      "Iteration 64 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_64_seed-0.pkl\n",
      "Starting BO Iteration: 74 / 100\n",
      "Iteration 65 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_65_seed-0.pkl\n",
      "Starting BO Iteration: 75 / 100\n",
      "Iteration 66 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_66_seed-0.pkl\n",
      "Starting BO Iteration: 76 / 100\n",
      "Iteration 67 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_67_seed-0.pkl\n",
      "Starting BO Iteration: 77 / 100\n",
      "Iteration 68 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_68_seed-0.pkl\n",
      "Starting BO Iteration: 78 / 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_69_seed-0.pkl\n",
      "Starting BO Iteration: 79 / 100\n",
      "Iteration 70 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_70_seed-0.pkl\n",
      "Starting BO Iteration: 80 / 100\n",
      "Iteration 71 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_71_seed-0.pkl\n",
      "Starting BO Iteration: 81 / 100\n",
      "Iteration 72 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_72_seed-0.pkl\n",
      "Starting BO Iteration: 82 / 100\n",
      "Iteration 73 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_73_seed-0.pkl\n",
      "Starting BO Iteration: 83 / 100\n",
      "Iteration 74 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_74_seed-0.pkl\n",
      "Starting BO Iteration: 84 / 100\n",
      "Iteration 75 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_75_seed-0.pkl\n",
      "Starting BO Iteration: 85 / 100\n",
      "Iteration 76 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_76_seed-0.pkl\n",
      "Starting BO Iteration: 86 / 100\n",
      "Iteration 77 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_77_seed-0.pkl\n",
      "Starting BO Iteration: 87 / 100\n",
      "Iteration 78 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_78_seed-0.pkl\n",
      "Starting BO Iteration: 88 / 100\n",
      "Iteration 79 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_79_seed-0.pkl\n",
      "Starting BO Iteration: 89 / 100\n",
      "Iteration 80 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_80_seed-0.pkl\n",
      "Starting BO Iteration: 90 / 100\n",
      "Iteration 81 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_81_seed-0.pkl\n",
      "Starting BO Iteration: 91 / 100\n",
      "Iteration 82 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_82_seed-0.pkl\n",
      "Starting BO Iteration: 92 / 100\n",
      "Iteration 83 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_83_seed-0.pkl\n",
      "Starting BO Iteration: 93 / 100\n",
      "Iteration 84 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_84_seed-0.pkl\n",
      "Starting BO Iteration: 94 / 100\n",
      "Iteration 85 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_85_seed-0.pkl\n",
      "Starting BO Iteration: 95 / 100\n",
      "Iteration 86 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_86_seed-0.pkl\n",
      "Starting BO Iteration: 96 / 100\n",
      "Iteration 87 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_87_seed-0.pkl\n",
      "Starting BO Iteration: 97 / 100\n",
      "Iteration 88 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_88_seed-0.pkl\n",
      "Starting BO Iteration: 98 / 100\n",
      "Iteration 89 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_89_seed-0.pkl\n",
      "Starting BO Iteration: 99 / 100\n",
      "Iteration 90 saved as ./results/New_E_sparsity-CMI10_model-GP_acq-EI_iter_90_seed-0.pkl\n",
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 10 samples from the search space.\n",
      "Starting BO Iteration: 10 / 100\n",
      "> \u001b[0;32m/tmp/ipykernel_1423267/2509397791.py\u001b[0m(583)\u001b[0;36msafe_eval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    581 \u001b[0;31m  \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    582 \u001b[0;31m  \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 583 \u001b[0;31m  \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    584 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    585 \u001b[0;31m      \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1423267/901931656.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Step 5: Run the optimization for a few iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Running optimization for the first time:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmol_dais\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1423267/2509397791.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;31m# Get the next sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                 \u001b[0mnext_sample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0mnext_sample_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_sample_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1423267/2509397791.py\u001b[0m in \u001b[0;36mget_sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m                         partitioning=partitioning, )\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                     \u001b[0macq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_acq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0mmax_acq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0mmax_acq_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1423267/2509397791.py\u001b[0m in \u001b[0;36msafe_eval\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m    581\u001b[0m   \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m   \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m       \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1423267/2509397791.py\u001b[0m in \u001b[0;36msafe_eval\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m    581\u001b[0m   \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m   \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m       \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/work/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/work/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Define the problem\n",
    "PS_problem = MolDAIS.Problem(smiles_search_space=smiles_list,\n",
    "                          targets=values,\n",
    "                          descriptors_search_space = descriptors_search_space,\n",
    "                          experiment_name=name)\n",
    "#PS_problem.compute_descriptors()\n",
    "\n",
    "\n",
    "r_list = []\n",
    "for kk in range(10):\n",
    "    # Step 3: Define optimizer parameters\n",
    "    optimizer_parameters = MolDAIS.OptimizerParameters(\n",
    "        sparsity_method='CMI',  # Use MIC as the sparsity method\n",
    "        acq_fun='EI',  # Use Expected Improvement (EI) as the acquisition function\n",
    "        num_sparsity_feats=10,  # Let's use 10 features for sparsity\n",
    "        total_budget=100,  # Number of iterations to run\n",
    "        initialization_budget=10,  # Number of initial points\n",
    "        seed=kk,  # Seed for reproducibility\n",
    "        multi_objective = True,\n",
    "    )\n",
    "\n",
    "    # Step 4: Create the optimization problem and the MolDAIS object\n",
    "    mol_dais = MolDAIS(problem=PS_problem, optimizer_parameters=optimizer_parameters)\n",
    "    \n",
    "    #checkpoint_filename = f'./results/MOO_E_sparsity-CMI10_model-GP_acq-EI_iter_40_seed-{kk}.pkl'\n",
    "    #mol_dais.configuration.load_from_checkpoint(checkpoint_filename)\n",
    "    \n",
    "    # Step 5: Run the optimization for a few iterations\n",
    "    print(\"\\n Running optimization for the first time:\")\n",
    "    mol_dais.configuration.optimize()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55ac25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4674b19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 100 samples from the search space.\n",
      "Iteration 0 saved as ./results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-0.pkl\n",
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 100 samples from the search space.\n",
      "Iteration 0 saved as ./results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-1.pkl\n",
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 100 samples from the search space.\n",
      "Iteration 0 saved as ./results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-2.pkl\n",
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 100 samples from the search space.\n",
      "Iteration 0 saved as ./results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-3.pkl\n",
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 100 samples from the search space.\n",
      "Iteration 0 saved as ./results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-4.pkl\n",
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 100 samples from the search space.\n",
      "Iteration 0 saved as ./results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-5.pkl\n",
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 100 samples from the search space.\n",
      "Iteration 0 saved as ./results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-6.pkl\n",
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 100 samples from the search space.\n",
      "Iteration 0 saved as ./results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-7.pkl\n",
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 100 samples from the search space.\n",
      "Iteration 0 saved as ./results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-8.pkl\n",
      "using second variable\n",
      "\n",
      " Running optimization for the first time:\n",
      "Initializing with random 100 samples from the search space.\n",
      "Iteration 0 saved as ./results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-9.pkl\n"
     ]
    }
   ],
   "source": [
    "# random sample\n",
    "\n",
    "# Step 2: Define the problem\n",
    "PS_problem = MolDAIS.Problem(smiles_search_space=smiles_list,\n",
    "                          targets=values,\n",
    "                          descriptors_search_space = descriptors_search_space,\n",
    "                          experiment_name=name)\n",
    "#PS_problem.compute_descriptors()\n",
    "\n",
    "\n",
    "r_list = []\n",
    "for kk in range(10):\n",
    "    # Step 3: Define optimizer parameters\n",
    "    optimizer_parameters = MolDAIS.OptimizerParameters(\n",
    "        sparsity_method='MI',  # Use MIC as the sparsity method\n",
    "        acq_fun='Random',  # Use Expected Improvement (EI) as the acquisition function\n",
    "        num_sparsity_feats=10,  # Let's use 10 features for sparsity\n",
    "        total_budget=100,  # Number of iterations to run\n",
    "        initialization_budget=10,  # Number of initial points\n",
    "        seed=kk,  # Seed for reproducibility\n",
    "        multi_objective = True,\n",
    "    )\n",
    "\n",
    "    # Step 4: Create the optimization problem and the MolDAIS object\n",
    "    mol_dais = MolDAIS(problem=PS_problem, optimizer_parameters=optimizer_parameters)\n",
    "    \n",
    "    # Step 5: Run the optimization for a few iterations\n",
    "    print(\"\\n Running optimization for the first time:\")\n",
    "    mol_dais.configuration.optimize()\n",
    "    mol_dais.configuration.save_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4839e62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = values\n",
    "hv = Y[:,1]*Y[:,0]\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.kdeplot(hv,cut=0, linewidth=4)\n",
    "plt.xlabel(r\"$-\\Delta G \\cdot E^o$\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figs/MOO_density.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "moldais_moo = []\n",
    "sgp_moo  = []\n",
    "rnd_moo  = []\n",
    "tmfp_moo = []\n",
    "\n",
    "\n",
    "for seed in range(10):\n",
    "    moldais_moo.append( torch.load(f'./mic_res/results/MOO_E_sparsity-CMI10_model-GP_acq-EI_iter_40_seed-{seed}.pkl')['best_values'])\n",
    "moldais = np.array(moldais_moo)\n",
    "moldais_m = moldais.mean(axis=0)\n",
    "moldais_s = moldais.std(axis=0)    \n",
    "    \n",
    "    \n",
    "    \n",
    "#for seed in range(10):    \n",
    "#    sgp_moo.append(torch.load(f'./mic_res/results/MOO_E_sparsity-MI10000_model-GP_acq-EI_iter_40_seed-{seed}.pkl')['best_values'])\n",
    "#sgp = np.array(sgp_moo)\n",
    "#sgp_m = sgp.mean(axis=0)\n",
    "#sgp_s = sgp.std(axis=0)        \n",
    "\n",
    "\n",
    "\n",
    "for seed in range(10):    \n",
    "    Yi = torch.load(f'./mic_res/results/MOO_E_sparsity-MI10_model-GP_acq-Random_iter_0_seed-{seed}.pkl')['y']\n",
    "    Y = Yi.prod(1)\n",
    "    ybest = [Y[:k].max().item() for k in range(10,50)]\n",
    "    rnd_moo.append(ybest)\n",
    "rnd = np.array(rnd_moo)\n",
    "rnd_m = rnd.mean(axis=0)\n",
    "rnd_s = rnd.std(axis=0)        \n",
    "\n",
    "    \n",
    "\n",
    "num_list = []\n",
    "df_g = pd.read_csv('./mic_res/results/Gauche_MOO.csv')['y1']\n",
    "for i in range(10):\n",
    "    str_list = df_g[i].replace('\\n','').replace('tensor([','').replace('Tensor([','').replace('       ','').replace('],dtype=torch.float64)','').replace('], dtype=torch.float64)','').replace(')','').split(', ')\n",
    "    num_list.append( np.array([float(x.replace(']', '')) for x in str_list]) )\n",
    "gauche = np.array(num_list)[:,:40]\n",
    "gauche_m = gauche.mean(axis=0)\n",
    "gauche_s = gauche.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot([0,41], [hv.max(),hv.max()], 'k:'  )\n",
    "\n",
    "plt.plot(np.arange(1,41), moldais_m, label='MolDAIS')\n",
    "plt.fill_between(np.arange(1,41),moldais_m+1.95*moldais_s/10**0.5,moldais_m-1.95*moldais_s/10**0.5, alpha=.3)\n",
    "\n",
    "#plt.plot(np.arange(1,41), sgp_m, label='SGP')\n",
    "#plt.fill_between(np.arange(1,41),sgp_m+1.95*sgp_s/10**0.5,sgp_m-1.95*sgp_s/10**0.5, alpha=.3)\n",
    "\n",
    "plt.plot(np.arange(1,41), rnd_m, label='Random')\n",
    "plt.fill_between(np.arange(1,41),rnd_m+1.95*rnd_s/10**0.5,rnd_m-1.95*rnd_s/10**0.5, alpha=.3)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(1,41),gauche_m, label='TM-FP')\n",
    "plt.fill_between(np.arange(1,41),gauche_m+1.95*gauche_s/10**0.5,gauche_m-1.95*gauche_s/10**0.5, alpha=.3)\n",
    "\n",
    "\n",
    "\n",
    "plt.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "\n",
    "#plt.yscale('exp')\n",
    "#plt.ylim(0)\n",
    "plt.xlim(1,40)\n",
    "plt.legend(fontsize=14, ncols = 1, )\n",
    "plt.ylabel(r\"$-\\Delta G \\cdot E^o$\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e278f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sorted_results5 = Res_dict_raw['random']\n",
    "\n",
    "\n",
    "seeds = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "sorted_results1 = Res_dict_raw['SAAS']\n",
    "sorted_results2 = Res_dict_raw['GP_full']\n",
    "sorted_results3 = Res_dict_raw['GP_PCA']\n",
    "sorted_results4 = Res_dict_raw['Linear']\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "col_name = r'Quantile scores of $m^* $'\n",
    "\n",
    "end = -1\n",
    "\n",
    "d =  pd.DataFrame({'Method':['MolDAIS']*len(seeds), col_name:CQ(moldais[:,end],hv)})\n",
    "df2 = pd.concat([df2, d])\n",
    "\n",
    "#d =  pd.DataFrame({'Method':['SGP']*len(seeds), col_name:CQ(sgp[:,end],hv)})\n",
    "#df2 = pd.concat([df2, d])\n",
    "\n",
    "\n",
    "d =  pd.DataFrame({'Method':['Random']*len(seeds), col_name:CQ(rnd[:,end],hv)})\n",
    "df2 = pd.concat([df2, d])\n",
    "\n",
    "\n",
    "\n",
    "d =  pd.DataFrame({'Method':['TM-FP']*len(seeds), col_name:CQ(gauche[:,end],hv)})\n",
    "df2 = pd.concat([df2, d])\n",
    "\n",
    "\n",
    "C = [  'tab:cyan',\n",
    " 'tab:orange',\n",
    " 'tab:green',\n",
    " 'tab:red',\n",
    " 'tab:purple',\n",
    "\n",
    "         ]\n",
    "\n",
    "plt.figure(figsize=(13,9))\n",
    "for i,ls in enumerate(range(4)):\n",
    "  plt.axvline(i, color=C[i], linestyle='-', zorder=0, linewidth=184, alpha=.15)\n",
    "#plt.axhline(1,color='k', linestyle=':',zorder=0)\n",
    "#plt.axhline(.99,color='k', linestyle=':',zorder=0)\n",
    "#plt.axhline(.98,color='k', linestyle=':',zorder=0)\n",
    "plt.axhline(1,color='k', linestyle=':',zorder=0)\n",
    "\n",
    "sns.violinplot(data=df2, x='Method', y=col_name, cut=0, inner=\"box\", scale='width', palette=C, linewidth=3,linecolor='k')\n",
    "#plt.title('Lipophilicity')\n",
    "plt.xlabel(None)\n",
    "#plt.ylim(.93,1.001)\n",
    "#density_norm{“area”, “count”, “width”}\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figs/MOO_finalVals.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (vanilla)",
   "language": "python",
   "name": "vanilla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
